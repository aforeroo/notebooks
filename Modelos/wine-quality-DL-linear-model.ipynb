{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Wine Quality Dataset and Neural Networks with Keras\n",
    "\n",
    "linear units. We saw that a model of just one linear unit will fit a linear function to a dataset (equivalent to linear regression). In this exercise, you'll build a linear model\n",
    "\n",
    "se busca entrenar un modelo lineal de redes nueronales el cual estime(regression) la calidad  `quality` de un vino a partir un conjunto de valores de sus features fisico-quimicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
       "0            7.4              0.70         0.00  ...       0.56      9.4        5\n",
       "1            7.8              0.88         0.00  ...       0.68      9.8        5\n",
       "2            7.8              0.76         0.04  ...       0.65      9.8        5\n",
       "3           11.2              0.28         0.56  ...       0.58      9.8        6\n",
       "4            7.4              0.70         0.00  ...       0.56      9.4        5\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "red_wine = pd.read_csv('../datasets/red-wine.csv')\n",
    "print(red_wine.info())\n",
    "red_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input shape\n",
    "\n",
    "se definen el numero de features o inputs a las neuronas del modelo, usaremos todas las 11 features de un vino excluyendo el `quality` ya que es el target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#create a network with 1 linear input\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1, input_shape=[11])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesos o Tensores (Tensors)\n",
    "\n",
    "Internally, Keras represents the weights of a neural network with tensors. Tensors are basically TensorFlow's version of a Numpy array with a few differences that make them better suited to deep learning. One of the most important is that tensors are compatible with GPU and TPU) accelerators. TPUs, in fact, are designed specifically for tensor computations\n",
    "\n",
    "A model's weights are kept in its weights attribute as a list of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights\n",
      "<tf.Variable 'dense_33/kernel:0' shape=(11, 1) dtype=float32, numpy=\n",
      "array([[ 0.51499397],\n",
      "       [ 0.6153148 ],\n",
      "       [ 0.42795306],\n",
      "       [ 0.32087308],\n",
      "       [ 0.5128936 ],\n",
      "       [-0.16711211],\n",
      "       [-0.06104988],\n",
      "       [-0.2966209 ],\n",
      "       [ 0.18255806],\n",
      "       [ 0.6827747 ],\n",
      "       [-0.5919481 ]], dtype=float32)>\n",
      "\n",
      "Bias\n",
      "<tf.Variable 'dense_33/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "w, bias = model.weights\n",
    "print(\"Weights\\n{}\\n\\nBias\\n{}\".format(w, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there's one weight for each input (and a bias).  There doesn't seem to be any pattern to the values the weights have. Before the model is trained, the weights are set to random numbers (and the bias to 0.0). A neural network learns by finding better values for its weights\n",
    "\n",
    "Keras represents weights as tensors, but also uses tensors to represent data. When you set the input_shape argument, you are telling Keras the dimensions of the array it should expect for each example in the training data. Setting input_shape=[3] would create a network accepting vectors of length 3, like [0.2, 0.4, 0.6].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deep Neural Networks with sequential Layers\n",
    "\n",
    " to add non linearity to tackle more complex regression problems or recognice more deep and complex patterns and relationships in the data, we get a rectified linear unit or ReLU. (For this reason, it's common to call the rectifier function the \"ReLU function\".) Applying a ReLU activation to a linear unit means the output becomes max(0, w * x + b), this is a function that is aplied to a whole layer of neurons to increase the options of transformation of outputs and therefore increase the learning capacity of the model\n",
    "\n",
    " this funciton is called an activation function and it could be any transformation, ReLU es solo un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=512, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(units=512, activation='relu'),\n",
    "    layers.Dense(units=512, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual way of attaching an activation function to a Dense layer is to include it as part of the definition with the activation argument. Sometimes though you'll want to put some other layer between the Dense layer and its activation function.\n",
    "\n",
    " In this case, we can define the activation in its own Activation layer, like so:\n",
    "\n",
    " `layers.Dense(units=8),`\n",
    "\n",
    " `layers.Activation('relu'),`\n",
    "\n",
    " This is completely equivalent to the ordinary way: layers.Dense(units=8, activation='relu')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives to ReLU\n",
    "There is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with. (Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for more ideas.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Change 'relu' to 'elu', 'selu', 'swish'... \n",
    "# r something else in https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "activation_layer = layers.Activation('relu')\n",
    "\n",
    "x = tf.linspace(-3.0, 3.0, 100)\n",
    "y = activation_layer(x) # once created, a layer is callable just like a function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, y)\n",
    "plt.xlim(-3, 3)\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamieto de la red neuronal y Stochastic Gradient Descent\n",
    "\n",
    " When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. As with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target.\n",
    "\n",
    " If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function: Â¿how good the network's predictions are?\n",
    "\n",
    "we haven't seen how to tell a network what problem to solve. This is the job of the loss function.\n",
    "\n",
    "The loss function measures the disparity between the the target's true value and the value the model predicts.\n",
    "\n",
    "Different problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value, A common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
    "\n",
    "Besides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras). During training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Optimizer - Stochastic Gradient Descent Or how can the model change the tensors(weights) to get better estimations\n",
    "\n",
    "We've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\n",
    "\n",
    "Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
    "\n",
    "1. Sample some training data and run it through the network to make predictions.\n",
    "2. Measure the loss between the predictions and the true values.\n",
    "3. Finally, adjust the weights in a direction that makes the loss smaller.\n",
    "\n",
    "Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n",
    "\n",
    "![](https://i.imgur.com/rFI1tIk.gif)\n",
    "\n",
    "Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\n",
    "\n",
    "The animation shows the model of wine quality being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.\n",
    "\n",
    "### Why SGD?\n",
    "\n",
    "The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate and Batch Size\n",
    "\n",
    "Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
    "\n",
    "The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\n",
    "\n",
    "Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. ``Adam`` is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\n",
    "\n",
    "After defining a model, you can add a loss function and optimizer with the model's ``compile`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare Data: Sample and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>10.8</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.171</td>\n",
       "      <td>27.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.76</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.095</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.99854</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>9.1</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.063</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99516</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.84</td>\n",
       "      <td>11.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>10.2</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.053</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>12.2</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.075</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
       "1109           10.8             0.470         0.43  ...       0.76     10.8        6\n",
       "1032            8.1             0.820         0.00  ...       0.53      9.6        5\n",
       "1002            9.1             0.290         0.33  ...       0.84     11.7        7\n",
       "487            10.2             0.645         0.36  ...       0.42     10.0        6\n",
       "979            12.2             0.450         0.49  ...       0.63     10.4        5\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create training and validation splits\n",
    "df_training = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_validation = red_wine.drop(df_training.index)\n",
    "display(df_training.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "las escalas de los imputs son importantes vamos a escalar los valores de los features a [0, 1]\n",
    "\n",
    "neural networks tend to perform best when their inputs are on a common scale\n",
    "\n",
    "If your number X falls between A and B, and you would like Y to fall between C and D, you can apply the following linear transform:\n",
    "\n",
    "`Y = (X-A)/(B-A) * (D-C) + C`\n",
    "\n",
    "para el caso de mapear a [0, 1], esta expresion se convierte en:\n",
    "\n",
    "`Y = (X-A)/(B-A) `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>0.548673</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.265442</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.212014</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.257485</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>0.309735</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.138564</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.621880</td>\n",
       "      <td>0.488189</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.116438</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.078767</td>\n",
       "      <td>0.085142</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.074205</td>\n",
       "      <td>0.373715</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>0.305389</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.495575</td>\n",
       "      <td>0.359589</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.068447</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.226027</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.105175</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501468</td>\n",
       "      <td>0.307087</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  ...   alcohol  quality\n",
       "1109       0.548673          0.239726  ...  0.369231      0.6\n",
       "1032       0.309735          0.479452  ...  0.184615      0.4\n",
       "1002       0.398230          0.116438  ...  0.507692      0.8\n",
       "487        0.495575          0.359589  ...  0.246154      0.6\n",
       "979        0.672566          0.226027  ...  0.307692      0.4\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.106845</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.098940</td>\n",
       "      <td>0.567548</td>\n",
       "      <td>0.606299</td>\n",
       "      <td>0.137725</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.105175</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.190813</td>\n",
       "      <td>0.582232</td>\n",
       "      <td>0.330709</td>\n",
       "      <td>0.149701</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.238938</td>\n",
       "      <td>0.363014</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.088481</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.053004</td>\n",
       "      <td>0.332599</td>\n",
       "      <td>0.511811</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.256637</td>\n",
       "      <td>0.260274</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.356164</td>\n",
       "      <td>0.098497</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.339223</td>\n",
       "      <td>0.567548</td>\n",
       "      <td>0.480315</td>\n",
       "      <td>0.281437</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.345133</td>\n",
       "      <td>0.253425</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.215548</td>\n",
       "      <td>0.494126</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fixed acidity  volatile acidity  citric acid  ...  sulphates   alcohol  quality\n",
       "0        0.247788          0.397260         0.00  ...   0.137725  0.153846      0.4\n",
       "3        0.584071          0.109589         0.56  ...   0.149701  0.215385      0.6\n",
       "7        0.238938          0.363014         0.00  ...   0.083832  0.246154      0.8\n",
       "11       0.256637          0.260274         0.36  ...   0.281437  0.323077      0.4\n",
       "23       0.345133          0.253425         0.11  ...   0.119760  0.153846      0.4\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_ = red_wine.max(axis=0) #arreglo con los valores maximos de cada columna\n",
    "min_ = red_wine.min(axis=0) #arreglo con los valores minimos de cada columna\n",
    "df_training = (df_training - min_) / (max_ - min_) # mapear conjunto de entrenamiento\n",
    "df_validation = (df_validation - min_) / (max_ - min_) #mapear conjunto de validacion\n",
    "display(df_training.head(5))\n",
    "display(df_validation.head(5))\n",
    "\n",
    "# NO CORRER MULTIPLES VECES esta celda, correr el notebook completo en su lugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X_train = df_training.drop('quality', axis=1)   #features de entrenamiento\n",
    "X_valid = df_validation.drop('quality', axis=1) #features de validacion\n",
    "y_train = df_training['quality']    # valores esperados de entrenamiento (labels)\n",
    "y_valid = df_validation['quality']  # valores a estimar por el modelo\n",
    "# se le esta haciendo drop a una copia de los dataframes de training y validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.3559 - val_loss: 0.1294\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.1378 - val_loss: 0.1237\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.1280 - val_loss: 0.1174\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.1153 - val_loss: 0.1140\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.1160 - val_loss: 0.1073\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.1115 - val_loss: 0.1057\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.1084 - val_loss: 0.1061\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 115ms/step - loss: 0.1066 - val_loss: 0.1030\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.1050 - val_loss: 0.1059\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.1042 - val_loss: 0.1009\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.1033 - val_loss: 0.1026\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0989 - val_loss: 0.0999\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.0992 - val_loss: 0.0992\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.0989 - val_loss: 0.0999\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.0965 - val_loss: 0.1030\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.1002 - val_loss: 0.1069\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.0979 - val_loss: 0.0985\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0984 - val_loss: 0.0960\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 113ms/step - loss: 0.0964 - val_loss: 0.1065\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.0961 - val_loss: 0.0962\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.0949 - val_loss: 0.0956\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0945 - val_loss: 0.0974\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.0921 - val_loss: 0.0953\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.0900 - val_loss: 0.0954\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.0924 - val_loss: 0.0995\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0922 - val_loss: 0.0972\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0883 - val_loss: 0.0956\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0878 - val_loss: 0.0976\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.0885 - val_loss: 0.0952\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0893 - val_loss: 0.0955\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0904 - val_loss: 0.0953\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.0906 - val_loss: 0.0943\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.0888 - val_loss: 0.0973\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.0881 - val_loss: 0.0986\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0876 - val_loss: 0.0962\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.0854 - val_loss: 0.0951\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.0828 - val_loss: 0.0936\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.0829 - val_loss: 0.0946\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0815 - val_loss: 0.0974\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.0848 - val_loss: 0.0933\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0822 - val_loss: 0.0928\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0858 - val_loss: 0.0962\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 0.0807 - val_loss: 0.0925\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0803 - val_loss: 0.0936\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.0830 - val_loss: 0.0939\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0824 - val_loss: 0.0978\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0835 - val_loss: 0.0940\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0818 - val_loss: 0.0938\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.0840 - val_loss: 0.0944\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.0799 - val_loss: 0.0924\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0798 - val_loss: 0.0952\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.0867 - val_loss: 0.0957\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0831 - val_loss: 0.0945\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.0851 - val_loss: 0.0918\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0794 - val_loss: 0.1078\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.0869 - val_loss: 0.0986\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0847 - val_loss: 0.0935\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0813 - val_loss: 0.0939\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.0803 - val_loss: 0.0925\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0779 - val_loss: 0.0925\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0765 - val_loss: 0.0925\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0771 - val_loss: 0.0932\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0767 - val_loss: 0.0930\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.0770 - val_loss: 0.0937\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0778 - val_loss: 0.0941\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0785 - val_loss: 0.0929\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.0763 - val_loss: 0.0924\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 114ms/step - loss: 0.0737 - val_loss: 0.0927\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 0.0736 - val_loss: 0.0922\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 110ms/step - loss: 0.0753 - val_loss: 0.0942\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0782 - val_loss: 0.0924\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0754 - val_loss: 0.0930\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0739 - val_loss: 0.0920\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0737 - val_loss: 0.0932\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.0732 - val_loss: 0.0923\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0737 - val_loss: 0.0936\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0720 - val_loss: 0.0922\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.0732 - val_loss: 0.0950\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0725 - val_loss: 0.0929\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.0726 - val_loss: 0.0948\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.0737 - val_loss: 0.0962\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0744 - val_loss: 0.0994\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0723 - val_loss: 0.0984\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.0727 - val_loss: 0.0936\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 109ms/step - loss: 0.0714 - val_loss: 0.0960\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.0696 - val_loss: 0.0955\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.0694 - val_loss: 0.0942\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 108ms/step - loss: 0.0680 - val_loss: 0.0930\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.0676 - val_loss: 0.0940\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 102ms/step - loss: 0.0661 - val_loss: 0.0922\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.0688 - val_loss: 0.0958\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0682 - val_loss: 0.0936\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.0673 - val_loss: 0.0968\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0732 - val_loss: 0.0930\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0685 - val_loss: 0.0943\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0706 - val_loss: 0.1025\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.0818 - val_loss: 0.0963\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.0733 - val_loss: 0.1004\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.0787 - val_loss: 0.0918\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 0.0659 - val_loss: 0.0921\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0658 - val_loss: 0.0937\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0673 - val_loss: 0.0941\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.0674 - val_loss: 0.0989\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0715 - val_loss: 0.0938\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0657 - val_loss: 0.0947\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0635 - val_loss: 0.0944\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 102ms/step - loss: 0.0619 - val_loss: 0.0928\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0619 - val_loss: 0.0938\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0619 - val_loss: 0.0945\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0630 - val_loss: 0.0971\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0631 - val_loss: 0.0986\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 109ms/step - loss: 0.0626 - val_loss: 0.0951\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.0629 - val_loss: 0.0973\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0627 - val_loss: 0.0931\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0610 - val_loss: 0.0954\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0627 - val_loss: 0.0942\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0633 - val_loss: 0.0945\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0618 - val_loss: 0.0949\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0638 - val_loss: 0.0943\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0613 - val_loss: 0.0937\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.0631 - val_loss: 0.1042\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0687 - val_loss: 0.1077\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.0726 - val_loss: 0.1010\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0661 - val_loss: 0.0981\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.0630 - val_loss: 0.0972\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.0655 - val_loss: 0.0988\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.0622 - val_loss: 0.0948\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0639 - val_loss: 0.0934\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0626 - val_loss: 0.0939\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.0643 - val_loss: 0.0954\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0613 - val_loss: 0.0952\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0602 - val_loss: 0.0929\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 0.0572 - val_loss: 0.0969\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0590 - val_loss: 0.0956\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.0569 - val_loss: 0.0929\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0582 - val_loss: 0.0954\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0585 - val_loss: 0.0940\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.0556 - val_loss: 0.0955\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0553 - val_loss: 0.0964\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.0548 - val_loss: 0.0954\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0552 - val_loss: 0.1003\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0598 - val_loss: 0.0950\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0579 - val_loss: 0.0934\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0542 - val_loss: 0.0950\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0538 - val_loss: 0.0943\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0540 - val_loss: 0.0956\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.0543 - val_loss: 0.0952\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0523 - val_loss: 0.0927\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0546 - val_loss: 0.0946\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0583 - val_loss: 0.1051\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0640 - val_loss: 0.0989\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0575 - val_loss: 0.0978\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0600 - val_loss: 0.0965\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 0.0581 - val_loss: 0.0944\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0533 - val_loss: 0.0938\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0512 - val_loss: 0.0981\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0590 - val_loss: 0.0974\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.0546 - val_loss: 0.0951\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0538 - val_loss: 0.0967\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.0543 - val_loss: 0.0931\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0542 - val_loss: 0.0965\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.0533 - val_loss: 0.0971\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.0549 - val_loss: 0.0946\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0560 - val_loss: 0.0970\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 105ms/step - loss: 0.0569 - val_loss: 0.0955\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0539 - val_loss: 0.0993\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0534 - val_loss: 0.0971\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.0531 - val_loss: 0.0950\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.0524 - val_loss: 0.0952\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0488 - val_loss: 0.0957\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.0483 - val_loss: 0.0957\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.0503 - val_loss: 0.0939\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0541 - val_loss: 0.0966\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 105ms/step - loss: 0.0515 - val_loss: 0.0970\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0506 - val_loss: 0.0950\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.0472 - val_loss: 0.0949\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.0472 - val_loss: 0.0960\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0488 - val_loss: 0.0966\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0476 - val_loss: 0.0969\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.0469 - val_loss: 0.0941\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0467 - val_loss: 0.0982\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0521 - val_loss: 0.0947\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.0533 - val_loss: 0.0972\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0543 - val_loss: 0.1052\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0607 - val_loss: 0.1072\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.0618 - val_loss: 0.0995\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.0513 - val_loss: 0.1040\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0597 - val_loss: 0.1008\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0546 - val_loss: 0.0973\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.0499 - val_loss: 0.0953\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0460 - val_loss: 0.0981\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.0475 - val_loss: 0.0964\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.0497 - val_loss: 0.0956\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0470 - val_loss: 0.0949\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0463 - val_loss: 0.0996\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.0486 - val_loss: 0.0970\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.0472 - val_loss: 0.0961\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0463 - val_loss: 0.0953\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0445 - val_loss: 0.0964\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0440 - val_loss: 0.0966\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 12 times all the way through the dataset (the epochs).\n",
    "\n",
    " let see the change of MAE loss though the difetents iterations of the traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the training history to a dataframe\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# use Pandas native plot method\n",
    "history_df['loss'].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv_notebooks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1b8d450b85d85de315d1ae1e2a65745e3b7df163ada4a1560eb28bd8feedc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
